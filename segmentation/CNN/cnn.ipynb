{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nes2YlAkGLr4",
        "outputId": "d78efff1-d16a-4713-e2a8-021d93592d4d"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pillow scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYuwaql_BZOi",
        "outputId": "038fde09-38ed-4a26-ae4f-5552ac8ed99d"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWd5i6W7I2oG",
        "outputId": "81bde79e-3552-4485-9328-6b5af3189699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_uW4-dmUvB1"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/content/drive/MyDrive/cnn-dhp/images\"\n",
        "mask_dir = \"/content/drive/MyDrive/cnn-dhp/masks\"\n",
        "output_dir = \"predictions\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Collect image and mask paths\n",
        "image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "mask_paths = []\n",
        "\n",
        "for img_path in image_paths:\n",
        "    filename = os.path.basename(img_path)\n",
        "    mask_filename = os.path.splitext(filename)[0] + \".png\"\n",
        "    mask_path = os.path.join(mask_dir, mask_filename)\n",
        "    if os.path.exists(mask_path):\n",
        "        mask_paths.append(mask_path)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Mask for {filename} not found at {mask_path}\")\n",
        "\n",
        "print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks.\")\n",
        "\n",
        "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
        "    image_paths, mask_paths, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Training images: {len(train_imgs)}, Validation images: {len(val_imgs)}\")\n",
        "\n",
        "# Full-size image dimensions\n",
        "CROP_SIZE = 4032  # Updated to be divisible by 16\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Preprocessing and augmentation\n",
        "@tf.function\n",
        "def preprocess_full_image(image_path, mask_path):\n",
        "    image = tf.io.decode_png(tf.io.read_file(image_path), channels=3)\n",
        "    mask = tf.io.decode_png(tf.io.read_file(mask_path), channels=1)\n",
        "\n",
        "    image = tf.image.resize(image, [CROP_SIZE, CROP_SIZE], method='bilinear')\n",
        "    mask = tf.image.resize(mask, [CROP_SIZE, CROP_SIZE], method='nearest')\n",
        "\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    mask = tf.cast(mask, tf.float32) / 255.0\n",
        "    mask = tf.where(mask >= 0.5, 1.0, 0.0)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "@tf.function\n",
        "def augment_image(image, mask):\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_up_down(image)\n",
        "        mask = tf.image.flip_up_down(mask)\n",
        "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "    image = tf.image.rot90(image, k)\n",
        "    mask = tf.image.rot90(mask, k)\n",
        "    image = tf.image.random_brightness(image, 0.1)\n",
        "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
        "    return image, mask\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_imgs, train_masks))\n",
        "train_ds = train_ds.map(preprocess_full_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(32).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_imgs, val_masks))\n",
        "val_ds = val_ds.map(preprocess_full_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# U-Net architecture\n",
        "def conv_block(x, num_filters):\n",
        "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    c1 = conv_block(inputs, 16)\n",
        "    p1 = layers.MaxPooling2D()(c1)\n",
        "\n",
        "    c2 = conv_block(p1, 32)\n",
        "    p2 = layers.MaxPooling2D()(c2)\n",
        "\n",
        "    c3 = conv_block(p2, 64)\n",
        "    p3 = layers.MaxPooling2D()(c3)\n",
        "\n",
        "    c4 = conv_block(p3, 128)\n",
        "    p4 = layers.MaxPooling2D()(c4)\n",
        "\n",
        "    c5 = conv_block(p4, 256)\n",
        "\n",
        "    u6 = layers.UpSampling2D()(c5)\n",
        "    u6 = layers.Concatenate()([u6, c4])\n",
        "    c6 = conv_block(u6, 128)\n",
        "\n",
        "    u7 = layers.UpSampling2D()(c6)\n",
        "    u7 = layers.Concatenate()([u7, c3])\n",
        "    c7 = conv_block(u7, 64)\n",
        "\n",
        "    u8 = layers.UpSampling2D()(c7)\n",
        "    u8 = layers.Concatenate()([u8, c2])\n",
        "    c8 = conv_block(u8, 32)\n",
        "\n",
        "    u9 = layers.UpSampling2D()(c8)\n",
        "    u9 = layers.Concatenate()([u9, c1])\n",
        "    c9 = conv_block(u9, 16)\n",
        "\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c9)\n",
        "\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "model = build_unet((CROP_SIZE, CROP_SIZE, 3))\n",
        "\n",
        "# Custom Dice + BCE Loss\n",
        "from tensorflow.keras import backend as K\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1e-6\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return 1 - ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    dsc = dice_loss(y_true, y_pred)\n",
        "    return bce + dsc\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(1e-3), loss=combined_loss, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = callbacks.ModelCheckpoint(\"best_model_unet.keras\", monitor='val_loss', save_best_only=True)\n",
        "\n",
        "EPOCHS = 30\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[early_stop, checkpoint])\n",
        "\n",
        "# Evaluate and Save predictions\n",
        "iou_scores, precisions, recalls, f1s = [], [], [], []\n",
        "\n",
        "for img_path, mask_path in zip(val_imgs, val_masks):\n",
        "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    msk = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "\n",
        "    img = np.array(Image.fromarray(img).resize((CROP_SIZE, CROP_SIZE)))\n",
        "    msk = np.array(Image.fromarray(msk).resize((CROP_SIZE, CROP_SIZE)))\n",
        "\n",
        "    img_tensor = tf.expand_dims(tf.cast(img, tf.float32) / 255.0, axis=0)\n",
        "    pred = model.predict(img_tensor, verbose=0)[0]\n",
        "    pred_mask = (pred.squeeze() >= 0.5).astype(np.uint8) * 255\n",
        "\n",
        "    pred_bin = (pred_mask >= 128).flatten()\n",
        "    mask_bin = (msk >= 128).flatten()\n",
        "\n",
        "    intersection = np.logical_and(pred_bin, mask_bin).sum()\n",
        "    union = np.logical_or(pred_bin, mask_bin).sum()\n",
        "    iou = intersection / union if union != 0 else 1.0\n",
        "    iou_scores.append(iou)\n",
        "\n",
        "    precisions.append(precision_score(mask_bin, pred_bin, zero_division=1))\n",
        "    recalls.append(recall_score(mask_bin, pred_bin, zero_division=1))\n",
        "    f1s.append(f1_score(mask_bin, pred_bin, zero_division=1))\n",
        "\n",
        "    name = os.path.splitext(os.path.basename(img_path))[0] + \"_pred.png\"\n",
        "    Image.fromarray(pred_mask).save(os.path.join(output_dir, name))\n",
        "\n",
        "print(f\"\\nEvaluation Metrics on Validation Set:\")\n",
        "print(f\"Mean IoU:       {np.mean(iou_scores):.4f}\")\n",
        "print(f\"Mean Precision: {np.mean(precisions):.4f}\")\n",
        "print(f\"Mean Recall:    {np.mean(recalls):.4f}\")\n",
        "print(f\"Mean F1 Score:  {np.mean(f1s):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
